---
layout: lpost
title: 机器学习(2)-线性回归
categories: [ML]
---

* content
{:toc}


###  线性回归

#### Hypothesis

线性回归问题中，我们可以对我们的特征求权重和，然后将我们的权重和作为我们的预测

我们假设特征向量为$x = (x_0, x_1, ..., x_d)$， 那么我么你的权重和为:


$$
y \approx \sum_{i=0}^dw_ix_i
$$


通过向量表示，我们的线性回归hypothesis为：$h(x) = w^Tx$。



#### Error Measure

对于线性回归问题，我们一般使用squared error。

![image-20200312161254612](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312161254612.png)

为了实现机器学习，我们的目标就是最小化$E_{in}(w)$

#### 优化--最小二乘法

求解最小化$E_{in}(w)$的过程就是一个优化的过程。

在这里我们使用最小二乘法的方法对线性回归问题进行优化求解w。其最终结果为$w = (X^TX)^{-1}X^Ty$

[最小二乘法-百度百科](https://baike.baidu.com/item/最小二乘法)

其推理过程如下：

1. 首先将$E_{in}(w)$转化为矩阵的形式。

   ![image-20200312162259064](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312162259064.png)

2. 求偏导

   一般而言，此类线性回归问题是一个凸函数，这意味着，我们找到一阶导数为零的位置就找到了最优解
   ![image-20200312162747769](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312162747769.png)

   我们首先将$E_{in}(w)$展开

   ![image-20200312164123051](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312164123051.png)

   那么我们求解的梯度为:

   ![image-20200312164149616](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312164149616.png)

   令其等于0,则可以求得:

$$
   W_{LIN} = (X^TX)^{-1}X^Ty = X^+y
$$

 一般情况下，由于样本数量N一般远大于样本特征维度d+1， 所以能够保证矩阵的逆是存在的，我们成这样的矩阵为非奇异矩阵。

#### 线性回归的可行性

机器学习需要保证两个问题一个是$E_{in }$最小， 一个是$E_{in} \approx E_{out}$， 通过最小二乘法，我们可以保证第一个条件，我们主要要分析第二个条件。

使用$E_{in}$的均值来求解。我们抽取不同的样本，然后分别求解$E_{in }$，最后求解平均即可。

![image-20200312165512953](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312165512953.png)

我们令$H = XX^+$， 并称它为帽子函数。则

![image-20200312165922709](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312165922709.png)

我们可以通过几何图形的角度进一步求解$E_{in}$。(推理过程见笔记或课程)最终我们可以求出：

![image-20200312170600177](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312170600177.png)

其学习曲线为：

![image-20200312170655929](../posts/2020-03-10-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20200312170655929.png)

随着N的增加，两者越来越靠近。

#### 参考

[机器学习基石-coursera](https://www.coursera.org/learn/ntumlone-algorithmicfoundations/home/welcome)

[RedstoneWill的github笔记](https://github.com/RedstoneWill/HsuanTienLin_MachineLearning)